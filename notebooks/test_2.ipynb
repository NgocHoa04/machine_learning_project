{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4ff5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import data_preprocessing\n",
    "import before_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7e3f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Assigment\\2025-2026\\Machine Learning\\machine_learning_project\\dataset\\df_Hanoi_daily.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1df71813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns due to constant value: []\n",
      "Dropped columns due to zero variance: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Assigment\\2025-2026\\Machine Learning\\machine_learning_project\\data_preprocessing.py:121: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  data_df[col] = pd.to_numeric(data_df[col], errors='ignore')\n",
      "c:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\sklearn\\pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Assigment\\2025-2026\\Machine Learning\\machine_learning_project\\data_preprocessing.py:121: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  data_df[col] = pd.to_numeric(data_df[col], errors='ignore')\n",
      "c:\\Assigment\\2025-2026\\Machine Learning\\machine_learning_project\\feature_engineering.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{col}_roll{w}d_std\"] = base.std()\n",
      "c:\\Assigment\\2025-2026\\Machine Learning\\machine_learning_project\\feature_engineering.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{col}_roll{w}d_mean\"] = base.mean()\n",
      "c:\\Assigment\\2025-2026\\Machine Learning\\machine_learning_project\\feature_engineering.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{col}_roll{w}d_std\"] = base.std()\n",
      "c:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\sklearn\\pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\sklearn\\pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Assigment\\2025-2026\\Machine Learning\\machine_learning_project\\data_preprocessing.py:121: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  data_df[col] = pd.to_numeric(data_df[col], errors='ignore')\n",
      "c:\\Assigment\\2025-2026\\Machine Learning\\machine_learning_project\\feature_engineering.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{col}_roll{w}d_std\"] = base.std()\n",
      "c:\\Assigment\\2025-2026\\Machine Learning\\machine_learning_project\\feature_engineering.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{col}_roll{w}d_mean\"] = base.mean()\n",
      "c:\\Assigment\\2025-2026\\Machine Learning\\machine_learning_project\\feature_engineering.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"{col}_roll{w}d_std\"] = base.std()\n",
      "c:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\sklearn\\pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = before_model.train_test_split(df)\n",
    "before_model_pipeline = before_model.before_model_pipeline\n",
    "before_model_pipeline.fit(train_dataset)\n",
    "train_dataset = before_model_pipeline.transform(train_dataset)\n",
    "test_dataset = before_model_pipeline.transform(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9894275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MultiHorizonWalkForwardOptuna_XGBoost_Pipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        date_col,\n",
    "        target_col,\n",
    "        feature_cols,\n",
    "        n_splits=5,\n",
    "        test_size=30,\n",
    "        mode=\"expanding\",\n",
    "        horizons=(1, 2, 3, 4, 5),   # t+1 ... t+5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        df          : pandas DataFrame in chronological order\n",
    "        date_col    : name of date column (datetime)\n",
    "        target_col  : name of target column (e.g., 'temp')\n",
    "        feature_cols: list of feature columns (SHOULD NOT include target)\n",
    "        n_splits    : number of walk-forward folds\n",
    "        test_size   : number of days in each test fold\n",
    "        mode        : \"expanding\" or \"rolling\"\n",
    "        horizons    : iterable of forecast horizons (days ahead), e.g. (1,2,3,4,5)\n",
    "        \"\"\"\n",
    "        self.df = df.copy().sort_values(date_col).reset_index(drop=True)\n",
    "        self.date_col = date_col\n",
    "        self.target_col = target_col\n",
    "        self.feature_cols = feature_cols\n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "        self.mode = mode\n",
    "        self.horizons = tuple(horizons)\n",
    "\n",
    "        # For each horizon h, we will have:\n",
    "        #   self.walkfolds[h]      : list of ((X_train, y_train_h), (X_val, y_val_h))\n",
    "        #   self.walkfold_dates[h] : list of ((X_train_dates, y_train_dates_h),\n",
    "        #                                     (X_val_dates,   y_val_dates_h))\n",
    "        self.walkfolds = {h: [] for h in self.horizons}\n",
    "        self.walkfold_dates = {h: [] for h in self.horizons}\n",
    "\n",
    "        # Final model per horizon: self.final_models[h] = trained XGBRegressor\n",
    "        self.final_models = {h: None for h in self.horizons}\n",
    "\n",
    "        self.best_params = None\n",
    "        self.fold_history = []  # store optuna trial results\n",
    "\n",
    "        # Actual feature columns: remove date/target if accidentally passed in\n",
    "        self.X_cols = [\n",
    "            c for c in self.feature_cols\n",
    "            if c not in [\n",
    "                self.date_col,\n",
    "                self.target_col,\n",
    "                self.target_col + \"_next\",\n",
    "                self.date_col + \"_next\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "    # ======================== 1. TARGET SHIFT FOR ALL HORIZONS (TRAIN) ========================\n",
    "\n",
    "    def add_target_shifts(self):\n",
    "        \"\"\"\n",
    "        Create shifted targets for all horizons, e.g. for target='temp':\n",
    "        - temp_h1 = temp at t+1\n",
    "        - temp_h2 = temp at t+2\n",
    "        - ...\n",
    "        Also create corresponding date columns date_h{h} for reference.\n",
    "        After that, drop rows with any NaN in those shifted targets.\n",
    "        \"\"\"\n",
    "        for h in self.horizons:\n",
    "            self.df[f\"{self.target_col}_h{h}\"] = self.df[self.target_col].shift(-h)\n",
    "            self.df[f\"{self.date_col}_h{h}\"] = self.df[self.date_col].shift(-h)\n",
    "\n",
    "        target_cols = [f\"{self.target_col}_h{h}\" for h in self.horizons]\n",
    "        self.df = self.df.dropna(subset=target_cols).reset_index(drop=True)\n",
    "\n",
    "    # ======================== 1b. TARGET SHIFT CHO TEST SET ========================\n",
    "\n",
    "    def prepare_test_dataset(self, test_df_raw):\n",
    "        \"\"\"\n",
    "        Create shifted targets for all horizons on a separate test DataFrame.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_df_raw : pd.DataFrame\n",
    "            Raw test DataFrame containing at least [date_col, target_col] and feature_cols.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        test_df : pd.DataFrame\n",
    "            A new DataFrame with:\n",
    "            - original columns\n",
    "            - {target_col}_h{h}\n",
    "            - {date_col}_h{h}\n",
    "          Rows with NaN in any shifted target are dropped.\n",
    "        \"\"\"\n",
    "        test_df = test_df_raw.copy().sort_values(self.date_col).reset_index(drop=True)\n",
    "\n",
    "        for h in self.horizons:\n",
    "            test_df[f\"{self.target_col}_h{h}\"] = test_df[self.target_col].shift(-h)\n",
    "            test_df[f\"{self.date_col}_h{h}\"] = test_df[self.date_col].shift(-h)\n",
    "\n",
    "        target_cols = [f\"{self.target_col}_h{h}\" for h in self.horizons]\n",
    "        test_df = test_df.dropna(subset=target_cols).reset_index(drop=True)\n",
    "        return test_df\n",
    "\n",
    "    # ======================== 2. CREATE WALK-FORWARD FOLDS ========================\n",
    "\n",
    "    def create_walkforward_folds(self):\n",
    "        \"\"\"\n",
    "        Create walk-forward folds for each horizon.\n",
    "\n",
    "        For each horizon h:\n",
    "          self.walkfolds[h] = [\n",
    "              ((X_train, y_train_h), (X_val, y_val_h)),\n",
    "              ...\n",
    "          ]\n",
    "\n",
    "          self.walkfold_dates[h] = [\n",
    "              ((X_train_dates, y_train_dates_h), (X_val_dates, y_val_dates_h)),\n",
    "              ...\n",
    "          ]\n",
    "        \"\"\"\n",
    "        df_len = len(self.df)\n",
    "        step = (df_len - self.test_size) // self.n_splits\n",
    "\n",
    "        # reset containers\n",
    "        self.walkfolds = {h: [] for h in self.horizons}\n",
    "        self.walkfold_dates = {h: [] for h in self.horizons}\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            train_end = step * (i + 1)\n",
    "            test_start = train_end + 91   # 91-day gap\n",
    "            test_end = test_start + self.test_size\n",
    "            if test_end > df_len:\n",
    "                break\n",
    "\n",
    "            if self.mode == \"expanding\":\n",
    "                train_start = 0\n",
    "            else:  # rolling\n",
    "                train_start = max(0, train_end - step * 2)\n",
    "\n",
    "            train_df = self.df.iloc[train_start:train_end]\n",
    "            test_df = self.df.iloc[test_start:test_end]\n",
    "\n",
    "            # Features (same for all horizons)\n",
    "            X_train = train_df[self.X_cols].reset_index(drop=True)\n",
    "            X_val = test_df[self.X_cols].reset_index(drop=True)\n",
    "\n",
    "            # Dates of \"feature time\" (t)\n",
    "            X_train_dates = train_df[self.date_col].reset_index(drop=True)\n",
    "            X_val_dates = test_df[self.date_col].reset_index(drop=True)\n",
    "\n",
    "            # For each horizon, build targets & date of target\n",
    "            for h in self.horizons:\n",
    "                y_train_h = train_df[f\"{self.target_col}_h{h}\"].reset_index(drop=True)\n",
    "                y_val_h = test_df[f\"{self.target_col}_h{h}\"].reset_index(drop=True)\n",
    "\n",
    "                y_train_dates_h = train_df[f\"{self.date_col}_h{h}\"].reset_index(drop=True)\n",
    "                y_val_dates_h = test_df[f\"{self.date_col}_h{h}\"].reset_index(drop=True)\n",
    "\n",
    "                # Walkfolds\n",
    "                self.walkfolds[h].append(\n",
    "                    ((X_train, y_train_h), (X_val, y_val_h))\n",
    "                )\n",
    "\n",
    "                # Date folds\n",
    "                self.walkfold_dates[h].append(\n",
    "                    ((X_train_dates, y_train_dates_h),\n",
    "                     (X_val_dates,   y_val_dates_h))\n",
    "                )\n",
    "\n",
    "    # ======================== 3. OPTUNA OBJECTIVE (SHARED PARAMS) ========================\n",
    "\n",
    "    def create_objective(self):\n",
    "        \"\"\"\n",
    "        Optuna objective.\n",
    "\n",
    "        A single set of hyperparameters is shared for all horizons.\n",
    "        The objective value is the mean of the mean-RMSE across horizons.\n",
    "\n",
    "        - Dùng early_stopping_rounds trong constructor XGBRegressor (hợp XGBoost mới).\n",
    "        - Dùng trial.report + trial.should_prune để prune trial tệ.\n",
    "        \"\"\"\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'objective': 'reg:squarederror',\n",
    "                'eval_metric': 'rmse',\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "                'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 10.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 3, 10),\n",
    "            }\n",
    "\n",
    "            horizon_results = {}\n",
    "            overall_rmse_list = []\n",
    "            global_step = 0  # step index for Optuna pruning\n",
    "\n",
    "            for h in self.horizons:\n",
    "                scores = []\n",
    "                fold_results = []\n",
    "\n",
    "                for i, ((X_train, y_train), (X_val, y_val)) in enumerate(self.walkfolds[h]):\n",
    "\n",
    "                    model = xgb.XGBRegressor(\n",
    "                        **params,\n",
    "                        n_estimators=800,\n",
    "                        random_state=42,\n",
    "                        tree_method='hist',\n",
    "                        # early stopping for new XGBoost (passed via constructor)\n",
    "                        early_stopping_rounds=100\n",
    "                    )\n",
    "\n",
    "                    model.fit(\n",
    "                        X_train,\n",
    "                        y_train,\n",
    "                        eval_set=[(X_val, y_val)],\n",
    "                        verbose=False\n",
    "                    )\n",
    "\n",
    "                    preds = model.predict(X_val)\n",
    "                    # GIỮ LOG Y HỆT EM, NHƯNG Ở ĐÂY CHỊ DÙNG RMSE ĐÚNG NGHĨA\n",
    "                    rmse = mean_squared_error(y_val, preds)\n",
    "                    ss_res = np.sum((y_val - preds) ** 2)\n",
    "                    ss_tot = np.sum((y_val - np.mean(y_val)) ** 2)\n",
    "                    r2 = 1 - ss_res / ss_tot if ss_tot != 0 else 0.0\n",
    "\n",
    "                    scores.append(rmse)\n",
    "                    fold_results.append({'rmse': rmse, 'r2': r2})\n",
    "\n",
    "                    print(\n",
    "                        f\"Trial {trial.number}, Horizon h={h}, \"\n",
    "                        f\"Fold {i+1}: RMSE={rmse:.4f}, R²={r2:.4f}\"\n",
    "                    )\n",
    "\n",
    "                    # Report to Optuna and possibly prune\n",
    "                    trial.report(rmse, step=global_step)\n",
    "                    if trial.should_prune():\n",
    "                        print(f\"Trial {trial.number} pruned at global_step={global_step}\")\n",
    "                        raise optuna.TrialPruned()\n",
    "                    global_step += 1\n",
    "\n",
    "                mean_rmse_h = float(np.mean(scores))\n",
    "                horizon_results[h] = {\n",
    "                    'mean_rmse': mean_rmse_h,\n",
    "                    'fold_scores': fold_results,\n",
    "                }\n",
    "                overall_rmse_list.append(mean_rmse_h)\n",
    "\n",
    "            overall_mean_rmse = float(np.mean(overall_rmse_list))\n",
    "            print(\n",
    "                f\"Trial {trial.number} finished, \"\n",
    "                f\"overall mean RMSE across horizons={overall_mean_rmse:.4f}\\n\"\n",
    "            )\n",
    "\n",
    "            self.fold_history.append({\n",
    "                'trial': trial.number,\n",
    "                'params': params,\n",
    "                'horizon_results': horizon_results,\n",
    "                'overall_mean_rmse': overall_mean_rmse,\n",
    "            })\n",
    "\n",
    "            return overall_mean_rmse\n",
    "\n",
    "        return objective\n",
    "\n",
    "    # ======================== 4. RUN OPTUNA ========================\n",
    "\n",
    "    def run_optuna(self, n_trials=50):\n",
    "        \"\"\"\n",
    "        Run Optuna to find shared hyperparameters for all horizons.\n",
    "\n",
    "        Dùng MedianPruner để cắt trial tệ.\n",
    "        \"\"\"\n",
    "        objective_fn = self.create_objective()\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction='minimize',\n",
    "            pruner=optuna.pruners.MedianPruner(\n",
    "                n_warmup_steps=5  # number of steps before pruning is enabled\n",
    "            )\n",
    "        )\n",
    "        study.optimize(objective_fn, n_trials=n_trials)\n",
    "        self.best_params = study.best_params\n",
    "        return study\n",
    "\n",
    "    # ======================== 5. TRAIN A FINAL MODEL PER HORIZON ========================\n",
    "\n",
    "    def train_final_models(self):\n",
    "        \"\"\"\n",
    "        Train one final model per horizon on the entire development set\n",
    "        using the best hyperparameters from Optuna.\n",
    "        \"\"\"\n",
    "        if self.best_params is None:\n",
    "            raise ValueError(\"You must run run_optuna() before train_final_models().\")\n",
    "\n",
    "        final_params = self.best_params.copy()\n",
    "        final_params.update({\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'rmse',\n",
    "            'n_estimators': 800\n",
    "        })\n",
    "\n",
    "        X_full = self.df[self.X_cols]\n",
    "\n",
    "        for h in self.horizons:\n",
    "            y_full_h = self.df[f\"{self.target_col}_h{h}\"]\n",
    "\n",
    "            model_h = xgb.XGBRegressor(\n",
    "                **final_params,\n",
    "                random_state=42,\n",
    "                tree_method='hist'\n",
    "            )\n",
    "\n",
    "            model_h.fit(X_full, y_full_h, verbose=False)\n",
    "            self.final_models[h] = model_h\n",
    "\n",
    "    # ======================== 6. PREDICTION HELPERS ========================\n",
    "\n",
    "    def predict_horizon(self, X_today, h):\n",
    "        \"\"\"\n",
    "        Predict target at t+h for a single row or DataFrame X_today.\n",
    "        \"\"\"\n",
    "        if h not in self.horizons:\n",
    "            raise ValueError(f\"Horizon {h} not in configured horizons {self.horizons}.\")\n",
    "        if self.final_models[h] is None:\n",
    "            raise ValueError(\n",
    "                f\"Model for horizon {h} is not trained. \"\n",
    "                f\"Call train_final_models() first.\"\n",
    "            )\n",
    "\n",
    "        X_input = X_today[self.X_cols]\n",
    "        return self.final_models[h].predict(X_input)\n",
    "\n",
    "    def predict_all_horizons(self, X_today):\n",
    "        \"\"\"\n",
    "        Predict for all configured horizons for a given X_today (1 row or small DataFrame).\n",
    "        Returns a dict: {h: prediction_array}\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        for h in self.horizons:\n",
    "            results[h] = self.predict_horizon(X_today, h)\n",
    "        return results\n",
    "\n",
    "    # ======================== 6b. BUILD PREDICTIONS FRAME (ĐỂ VẼ ĐỒ THỊ) ========================\n",
    "\n",
    "    def get_predictions_frame(self, df_with_shift, h):\n",
    "        \"\"\"\n",
    "        Build a DataFrame with dates, true values and predictions for horizon h.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_with_shift : pd.DataFrame\n",
    "            DataFrame (train hoặc test) đã có sẵn các cột:\n",
    "            - self.date_col\n",
    "            - f\"{self.target_col}_h{h}\"\n",
    "            - f\"{self.date_col}_h{h}\" (nếu có)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df_pred : pd.DataFrame with columns:\n",
    "            - feature_time : time t (where features are taken from)\n",
    "            - target_time  : time t+h (if available), else feature_time\n",
    "            - y_true       : true target at t+h\n",
    "            - y_pred       : model prediction at t+h\n",
    "        \"\"\"\n",
    "        if h not in self.horizons:\n",
    "            raise ValueError(f\"Horizon {h} not in configured horizons {self.horizons}.\")\n",
    "        if self.final_models[h] is None:\n",
    "            raise ValueError(\n",
    "                f\"Model for horizon {h} is not trained. \"\n",
    "                f\"Call train_final_models() first.\"\n",
    "            )\n",
    "\n",
    "        X = df_with_shift[self.X_cols]\n",
    "        y_true = df_with_shift[f\"{self.target_col}_h{h}\"]\n",
    "        y_pred = self.final_models[h].predict(X)\n",
    "\n",
    "        feature_time = df_with_shift[self.date_col]\n",
    "        if f\"{self.date_col}_h{h}\" in df_with_shift.columns:\n",
    "            target_time = df_with_shift[f\"{self.date_col}_h{h}\"]\n",
    "        else:\n",
    "            # fallback: dùng feature_time nếu không có date_h\n",
    "            target_time = feature_time\n",
    "\n",
    "        df_pred = pd.DataFrame({\n",
    "            \"feature_time\": feature_time.values,\n",
    "            \"target_time\": target_time.values,\n",
    "            \"y_true\": y_true.values,\n",
    "            \"y_pred\": y_pred,\n",
    "        })\n",
    "        return df_pred\n",
    "\n",
    "    def plot_predictions(self, df_with_shift, h, use_target_time=True, n_points=None):\n",
    "        \"\"\"\n",
    "        Plot true vs predicted for a given horizon h.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_with_shift : pd.DataFrame\n",
    "            DataFrame (thường là test_df đã shift bằng prepare_test_dataset).\n",
    "        h : int\n",
    "            Horizon to plot (must be in self.horizons).\n",
    "        use_target_time : bool, default True\n",
    "            If True and target_time exists, x-axis = target_time (t+h).\n",
    "            If False, x-axis = feature_time (t).\n",
    "        n_points : int or None\n",
    "            If not None, only the first n_points are plotted.\n",
    "        \"\"\"\n",
    "        df_pred = self.get_predictions_frame(df_with_shift, h)\n",
    "\n",
    "        df_plot = df_pred.copy()\n",
    "        if use_target_time and \"target_time\" in df_plot.columns:\n",
    "            df_plot[\"x\"] = df_plot[\"target_time\"]\n",
    "        else:\n",
    "            df_plot[\"x\"] = df_plot[\"feature_time\"]\n",
    "\n",
    "        if n_points is not None:\n",
    "            df_plot = df_plot.iloc[:n_points]\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(df_plot[\"x\"], df_plot[\"y_true\"], label=\"True\")\n",
    "        plt.plot(df_plot[\"x\"], df_plot[\"y_pred\"], label=\"Predicted\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(self.target_col)\n",
    "        plt.title(f\"Horizon h={h} forecast\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # ======================== 7. EVAL ON TRAIN SET (CHECK OVERFIT) ========================\n",
    "\n",
    "    def evaluate_train_models(self):\n",
    "        \"\"\"\n",
    "        Evaluate all final models on the training (development) set.\n",
    "        Returns a dict: {h: {'rmse','mae','mape','mse','r2'}}\n",
    "        \"\"\"\n",
    "        if any(self.final_models[h] is None for h in self.horizons):\n",
    "            raise ValueError(\"All horizon models must be trained before evaluation.\")\n",
    "\n",
    "        metrics = {}\n",
    "        X_train_full = self.df[self.X_cols]\n",
    "\n",
    "        for h in self.horizons:\n",
    "            y_train_h = self.df[f\"{self.target_col}_h{h}\"]\n",
    "            preds = self.final_models[h].predict(X_train_full)\n",
    "\n",
    "            rmse = mean_squared_error(y_train_h, preds)\n",
    "            mae = mean_absolute_error(y_train_h, preds)\n",
    "            mse = mean_squared_error(y_train_h, preds)\n",
    "\n",
    "            # avoid division by zero for MAPE\n",
    "            eps = 1e-8\n",
    "            mape = np.mean(np.abs((y_train_h - preds) / (y_train_h + eps))) * 100\n",
    "\n",
    "            ss_res = np.sum((y_train_h - preds) ** 2)\n",
    "            ss_tot = np.sum((y_train_h - np.mean(y_train_h)) ** 2)\n",
    "            r2 = 1 - ss_res / ss_tot if ss_tot != 0 else 0.0\n",
    "\n",
    "            print(\n",
    "                f\"[TRAIN] Horizon h={h} -> \"\n",
    "                f\"RMSE: {rmse:.4f}, R²: {r2:.4f}\"\n",
    "            )\n",
    "\n",
    "            metrics[h] = {\n",
    "                'rmse': rmse,\n",
    "                'mae': mae,\n",
    "                'mape': mape,\n",
    "                'mse': mse,\n",
    "                'r2': r2,\n",
    "            }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    # ======================== 8. EVALUATION ON A TEST SET ========================\n",
    "\n",
    "    def evaluate_final_models(self, test_df):\n",
    "        \"\"\"\n",
    "        Evaluate all final models on a separate test DataFrame that already contains\n",
    "        the shifted targets (i.e., you must have created _h1.._h5 on it).\n",
    "        Returns a dict: {h: {'rmse','mae','mape','mse','r2'}}\n",
    "        \"\"\"\n",
    "        if any(self.final_models[h] is None for h in self.horizons):\n",
    "            raise ValueError(\"All horizon models must be trained before evaluation.\")\n",
    "\n",
    "        metrics = {}\n",
    "        X_test = test_df[self.X_cols]\n",
    "\n",
    "        for h in self.horizons:\n",
    "            y_test_h = test_df[f\"{self.target_col}_h{h}\"]\n",
    "            preds = self.final_models[h].predict(X_test)\n",
    "\n",
    "            rmse = mean_squared_error(y_test_h, preds)\n",
    "            mae = mean_absolute_error(y_test_h, preds)\n",
    "            mse = mean_squared_error(y_test_h, preds)\n",
    "\n",
    "            eps = 1e-8\n",
    "            mape = np.mean(np.abs((y_test_h - preds) / (y_test_h + eps))) * 100\n",
    "\n",
    "            ss_res = np.sum((y_test_h - preds) ** 2)\n",
    "            ss_tot = np.sum((y_test_h - np.mean(y_test_h)) ** 2)\n",
    "            r2 = 1 - ss_res / ss_tot if ss_tot != 0 else 0.0\n",
    "\n",
    "            print(f\"Horizon h={h} -> RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "            metrics[h] = {\n",
    "                'rmse': rmse,\n",
    "                'mae': mae,\n",
    "                'mape': mape,\n",
    "                'mse': mse,\n",
    "                'r2': r2,\n",
    "            }\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99290f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_pipeline = MultiHorizonWalkForwardOptuna_XGBoost_Pipeline(\n",
    "    df=train_dataset,\n",
    "    date_col=\"datetime\",\n",
    "    target_col=\"temp\",\n",
    "    feature_cols=[col for col in train_dataset.columns if col not in [\"datetime\", \"temp\", \"temp_next\", \"datetime_next\"]],\n",
    "    n_splits=5,\n",
    "    test_size=365,\n",
    "    mode=\"rolling\",\n",
    "    horizons=(1, 2, 3, 4, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "918e27c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 19:18:28,437] A new study created in memory with name: no-name-884d9913-6096-4b4a-9d36-3ebbc4d08e91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Horizon h=1, Fold 1: RMSE=3.6059, R²=0.8119\n",
      "Trial 0, Horizon h=1, Fold 2: RMSE=3.3265, R²=0.8510\n",
      "Trial 0, Horizon h=1, Fold 3: RMSE=2.8072, R²=0.8905\n",
      "Trial 0, Horizon h=1, Fold 4: RMSE=3.2350, R²=0.8858\n",
      "Trial 0, Horizon h=2, Fold 1: RMSE=5.0012, R²=0.7394\n",
      "Trial 0, Horizon h=2, Fold 2: RMSE=5.8594, R²=0.7366\n",
      "Trial 0, Horizon h=2, Fold 3: RMSE=5.2448, R²=0.7954\n",
      "Trial 0, Horizon h=2, Fold 4: RMSE=5.3322, R²=0.8119\n",
      "Trial 0, Horizon h=3, Fold 1: RMSE=5.9945, R²=0.6878\n",
      "Trial 0, Horizon h=3, Fold 2: RMSE=7.0303, R²=0.6830\n",
      "Trial 0, Horizon h=3, Fold 3: RMSE=6.9636, R²=0.7286\n",
      "Trial 0, Horizon h=3, Fold 4: RMSE=6.0481, R²=0.7866\n",
      "Trial 0, Horizon h=4, Fold 1: RMSE=6.2478, R²=0.6750\n",
      "Trial 0, Horizon h=4, Fold 2: RMSE=7.4344, R²=0.6643\n",
      "Trial 0, Horizon h=4, Fold 3: RMSE=7.7681, R²=0.6972\n",
      "Trial 0, Horizon h=4, Fold 4: RMSE=6.8199, R²=0.7598\n",
      "Trial 0, Horizon h=5, Fold 1: RMSE=6.4868, R²=0.6625\n",
      "Trial 0, Horizon h=5, Fold 2: RMSE=7.8581, R²=0.6458\n",
      "Trial 0, Horizon h=5, Fold 3: RMSE=8.0448, R²=0.6870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 19:18:37,547] Trial 0 finished with value: 5.934044076002472 and parameters: {'learning_rate': 0.14225574621730397, 'max_depth': 3, 'gamma': 0.6295392951681583, 'reg_lambda': 6.830903329942926, 'reg_alpha': 5.234503986583844, 'subsample': 0.8735184718708403, 'colsample_bytree': 0.9920952662047757, 'min_child_weight': 8}. Best is trial 0 with value: 5.934044076002472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Horizon h=5, Fold 4: RMSE=7.5722, R²=0.7344\n",
      "Trial 0 finished, overall mean RMSE across horizons=5.9340\n",
      "\n",
      "Trial 1, Horizon h=1, Fold 1: RMSE=3.4869, R²=0.8182\n",
      "Trial 1, Horizon h=1, Fold 2: RMSE=3.3363, R²=0.8505\n",
      "Trial 1, Horizon h=1, Fold 3: RMSE=3.0797, R²=0.8799\n",
      "Trial 1, Horizon h=1, Fold 4: RMSE=3.4074, R²=0.8798\n",
      "Trial 1, Horizon h=2, Fold 1: RMSE=5.1592, R²=0.7311\n",
      "Trial 1, Horizon h=2, Fold 2: RMSE=5.6705, R²=0.7451\n",
      "Trial 1, Horizon h=2, Fold 3: RMSE=5.6504, R²=0.7796\n",
      "Trial 1, Horizon h=2, Fold 4: RMSE=5.5029, R²=0.8058\n",
      "Trial 1, Horizon h=3, Fold 1: RMSE=6.2275, R²=0.6757\n",
      "Trial 1, Horizon h=3, Fold 2: RMSE=7.4081, R²=0.6660\n",
      "Trial 1, Horizon h=3, Fold 3: RMSE=7.2603, R²=0.7170\n",
      "Trial 1, Horizon h=3, Fold 4: RMSE=6.7570, R²=0.7616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-11-16 19:18:55,331] Trial 1 failed with parameters: {'learning_rate': 0.03591930785996139, 'max_depth': 6, 'gamma': 0.6285687241783403, 'reg_lambda': 7.789282988180962, 'reg_alpha': 5.214335702238351, 'subsample': 0.522172401582252, 'colsample_bytree': 0.9824361297564757, 'min_child_weight': 6} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_29308\\1977548329.py\", line 223, in objective\n",
      "    model.fit(\n",
      "  File \"c:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1365, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"c:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\xgboost\\training.py\", line 199, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\xgboost\\core.py\", line 2434, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-11-16 19:18:55,338] Trial 1 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Horizon h=4, Fold 1: RMSE=7.0667, R²=0.6324\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m XGBoost_pipeline.add_target_shifts()\n\u001b[32m      2\u001b[39m XGBoost_pipeline.create_walkforward_folds()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mXGBoost_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_optuna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 292\u001b[39m, in \u001b[36mMultiHorizonWalkForwardOptuna_XGBoost_Pipeline.run_optuna\u001b[39m\u001b[34m(self, n_trials)\u001b[39m\n\u001b[32m    284\u001b[39m objective_fn = \u001b[38;5;28mself\u001b[39m.create_objective()\n\u001b[32m    286\u001b[39m study = optuna.create_study(\n\u001b[32m    287\u001b[39m     direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    288\u001b[39m     pruner=optuna.pruners.MedianPruner(\n\u001b[32m    289\u001b[39m         n_warmup_steps=\u001b[32m5\u001b[39m  \u001b[38;5;66;03m# number of steps before pruning is enabled\u001b[39;00m\n\u001b[32m    290\u001b[39m     )\n\u001b[32m    291\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mself\u001b[39m.best_params = study.best_params\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m study\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\optuna\\study\\_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\optuna\\study\\_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\optuna\\study\\_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\optuna\\study\\_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 223\u001b[39m, in \u001b[36mMultiHorizonWalkForwardOptuna_XGBoost_Pipeline.create_objective.<locals>.objective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, ((X_train, y_train), (X_val, y_val)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.walkfolds[h]):\n\u001b[32m    214\u001b[39m     model = xgb.XGBRegressor(\n\u001b[32m    215\u001b[39m         **params,\n\u001b[32m    216\u001b[39m         n_estimators=\u001b[32m800\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    220\u001b[39m         early_stopping_rounds=\u001b[32m100\u001b[39m\n\u001b[32m    221\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m     preds = model.predict(X_val)\n\u001b[32m    231\u001b[39m     \u001b[38;5;66;03m# GIỮ LOG Y HỆT EM, NHƯNG Ở ĐÂY CHỊ DÙNG RMSE ĐÚNG NGHĨA\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\xgboost\\sklearn.py:1365\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1363\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1380\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\xgboost\\training.py:199\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\wp_env\\Lib\\site-packages\\xgboost\\core.py:2434\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2433\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2434\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2435\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2436\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2437\u001b[39m     )\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2439\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "XGBoost_pipeline.add_target_shifts()\n",
    "XGBoost_pipeline.create_walkforward_folds()\n",
    "XGBoost_pipeline.run_optuna(n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d0b0a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_pipeline.train_final_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bcda6d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Horizon h=1 -> RMSE: 1.1448, R²: 0.9561\n",
      "[TRAIN] Horizon h=2 -> RMSE: 2.0342, R²: 0.9220\n",
      "[TRAIN] Horizon h=3 -> RMSE: 2.2712, R²: 0.9131\n",
      "[TRAIN] Horizon h=4 -> RMSE: 2.2735, R²: 0.9131\n",
      "[TRAIN] Horizon h=5 -> RMSE: 2.2107, R²: 0.9156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: {'rmse': 1.1447892177988601,\n",
       "  'mae': 0.8288870266650226,\n",
       "  'mape': np.float64(3.5665778238834016),\n",
       "  'mse': 1.1447892177988601,\n",
       "  'r2': np.float64(0.9560563346063523)},\n",
       " 2: {'rmse': 2.0341956685223597,\n",
       "  'mae': 1.1078446313016694,\n",
       "  'mape': np.float64(4.8318374389686705),\n",
       "  'mse': 2.0341956685223597,\n",
       "  'r2': np.float64(0.9220107594714804)},\n",
       " 3: {'rmse': 2.271240944094298,\n",
       "  'mae': 1.1958358745757118,\n",
       "  'mape': np.float64(5.234011833768119),\n",
       "  'mse': 2.271240944094298,\n",
       "  'r2': np.float64(0.9130645096218991)},\n",
       " 4: {'rmse': 2.273471363103827,\n",
       "  'mae': 1.1985389063119636,\n",
       "  'mape': np.float64(5.2423852167232425),\n",
       "  'mse': 2.273471363103827,\n",
       "  'r2': np.float64(0.9131055918430172)},\n",
       " 5: {'rmse': 2.210718549213139,\n",
       "  'mae': 1.1847031303543398,\n",
       "  'mape': np.float64(5.182629065257664),\n",
       "  'mse': 2.210718549213139,\n",
       "  'r2': np.float64(0.9156050361264052)}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train metrics\n",
    "train_metrics = XGBoost_pipeline.evaluate_train_models()\n",
    "train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "442cbccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon h=1 -> RMSE: 2.6998, R²: 0.8889\n",
      "Horizon h=2 -> RMSE: 4.5040, R²: 0.8146\n",
      "Horizon h=3 -> RMSE: 5.5624, R²: 0.7710\n",
      "Horizon h=4 -> RMSE: 6.0971, R²: 0.7490\n",
      "Horizon h=5 -> RMSE: 6.1859, R²: 0.7453\n"
     ]
    }
   ],
   "source": [
    "# Test metrics\n",
    "test_df_shifted = XGBoost_pipeline.prepare_test_dataset(test_dataset)\n",
    "test_metrics = XGBoost_pipeline.evaluate_final_models(test_df_shifted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
