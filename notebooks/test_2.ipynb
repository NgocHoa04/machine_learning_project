{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d269814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "PROJECT_ROOT = r\"e:\\ML\\machine_learning_project\"\n",
    "SRC_PATH = os.path.join(PROJECT_ROOT, \"src\")\n",
    "\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"SRC_PATH:\", SRC_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import data.before_model as before_model      \n",
    "import data.data_preprocessing as data_preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e3f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'E:\\ML\\machine_learning_project\\dataset\\raw\\df_Hanoi_daily.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df71813",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = before_model.train_test_split(df)\n",
    "before_model_pipeline = before_model.before_model_pipeline\n",
    "before_model_pipeline.fit(train_dataset)\n",
    "train_dataset = before_model_pipeline.transform(train_dataset)\n",
    "test_dataset = before_model_pipeline.transform(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9894275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import optuna  \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MultiHorizonWalkForwardOptuna_XGBoost_Pipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        date_col,\n",
    "        target_col,\n",
    "        feature_cols,\n",
    "        n_splits=5,\n",
    "        test_size=30,\n",
    "        mode=\"expanding\",\n",
    "        horizons=(1, 2, 3, 4, 5),   # t+1 ... t+5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        df          : pandas DataFrame in chronological order\n",
    "        date_col    : name of date column (datetime)\n",
    "        target_col  : name of target column (e.g., 'temp')\n",
    "        feature_cols: list of feature columns (SHOULD NOT include target)\n",
    "        n_splits    : number of walk-forward folds\n",
    "        test_size   : number of days in each test fold\n",
    "        mode        : \"expanding\" or \"rolling\"\n",
    "        horizons    : iterable of forecast horizons (days ahead), e.g. (1,2,3,4,5)\n",
    "        \"\"\"\n",
    "        self.df = df.copy().sort_values(date_col).reset_index(drop=True)\n",
    "        self.date_col = date_col\n",
    "        self.target_col = target_col\n",
    "        self.feature_cols = feature_cols\n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "        self.mode = mode\n",
    "        self.horizons = tuple(horizons)\n",
    "\n",
    "        # For each horizon h, we will have:\n",
    "        #   self.walkfolds[h]      : list of ((X_train, y_train_h), (X_val, y_val_h))\n",
    "        #   self.walkfold_dates[h] : list of ((X_train_dates, y_train_dates_h),\n",
    "        #                                     (X_val_dates,   y_val_dates_h))\n",
    "        self.walkfolds = {h: [] for h in self.horizons}\n",
    "        self.walkfold_dates = {h: [] for h in self.horizons}\n",
    "\n",
    "        # Final model per horizon: self.final_models[h] = trained XGBRegressor\n",
    "        self.final_models = {h: None for h in self.horizons}\n",
    "\n",
    "        self.best_params = None\n",
    "        self.fold_history = []  # store optuna trial results\n",
    "\n",
    "        # Actual feature columns: remove date/target if accidentally passed in\n",
    "        self.X_cols = [\n",
    "            c for c in self.feature_cols\n",
    "            if c not in [\n",
    "                self.date_col,\n",
    "                self.target_col,\n",
    "                self.target_col + \"_next\",\n",
    "                self.date_col + \"_next\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "    # ======================== 1. TARGET SHIFT FOR ALL HORIZONS (TRAIN) ========================\n",
    "\n",
    "    def add_target_shifts(self):\n",
    "        \"\"\"\n",
    "        Create shifted targets for all horizons, e.g. for target='temp':\n",
    "        - temp_h1 = temp at t+1\n",
    "        - temp_h2 = temp at t+2\n",
    "        - ...\n",
    "        Also create corresponding date columns date_h{h} for reference.\n",
    "        After that, drop rows with any NaN in those shifted targets.\n",
    "        \"\"\"\n",
    "        for h in self.horizons:\n",
    "            self.df[f\"{self.target_col}_h{h}\"] = self.df[self.target_col].shift(-h)\n",
    "            self.df[f\"{self.date_col}_h{h}\"] = self.df[self.date_col].shift(-h)\n",
    "\n",
    "        target_cols = [f\"{self.target_col}_h{h}\" for h in self.horizons]\n",
    "        self.df = self.df.dropna(subset=target_cols).reset_index(drop=True)\n",
    "\n",
    "    # ======================== 1b. TARGET SHIFT CHO TEST SET ========================\n",
    "\n",
    "    def prepare_test_dataset(self, test_df_raw):\n",
    "        \"\"\"\n",
    "        Create shifted targets for all horizons on a separate test DataFrame.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_df_raw : pd.DataFrame\n",
    "            Raw test DataFrame containing at least [date_col, target_col] and feature_cols.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        test_df : pd.DataFrame\n",
    "            A new DataFrame with:\n",
    "            - original columns\n",
    "            - {target_col}_h{h}\n",
    "            - {date_col}_h{h}\n",
    "          Rows with NaN in any shifted target are dropped.\n",
    "        \"\"\"\n",
    "        test_df = test_df_raw.copy().sort_values(self.date_col).reset_index(drop=True)\n",
    "\n",
    "        for h in self.horizons:\n",
    "            test_df[f\"{self.target_col}_h{h}\"] = test_df[self.target_col].shift(-h)\n",
    "            test_df[f\"{self.date_col}_h{h}\"] = test_df[self.date_col].shift(-h)\n",
    "\n",
    "        target_cols = [f\"{self.target_col}_h{h}\" for h in self.horizons]\n",
    "        test_df = test_df.dropna(subset=target_cols).reset_index(drop=True)\n",
    "        return test_df\n",
    "\n",
    "    # ======================== 2. CREATE WALK-FORWARD FOLDS ========================\n",
    "\n",
    "    def create_walkforward_folds(self):\n",
    "        \"\"\"\n",
    "        Create walk-forward folds for each horizon.\n",
    "\n",
    "        For each horizon h:\n",
    "          self.walkfolds[h] = [\n",
    "              ((X_train, y_train_h), (X_val, y_val_h)),\n",
    "              ...\n",
    "          ]\n",
    "\n",
    "          self.walkfold_dates[h] = [\n",
    "              ((X_train_dates, y_train_dates_h), (X_val_dates, y_val_dates_h)),\n",
    "              ...\n",
    "          ]\n",
    "        \"\"\"\n",
    "        df_len = len(self.df)\n",
    "        step = (df_len - self.test_size) // self.n_splits\n",
    "\n",
    "        # reset containers\n",
    "        self.walkfolds = {h: [] for h in self.horizons}\n",
    "        self.walkfold_dates = {h: [] for h in self.horizons}\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            train_end = step * (i + 1)\n",
    "            test_start = train_end + 91   # 91-day gap\n",
    "            test_end = test_start + self.test_size\n",
    "            if test_end > df_len:\n",
    "                break\n",
    "\n",
    "            if self.mode == \"expanding\":\n",
    "                train_start = 0\n",
    "            else:  # rolling\n",
    "                train_start = max(0, train_end - step * 2)\n",
    "\n",
    "            train_df = self.df.iloc[train_start:train_end]\n",
    "            test_df = self.df.iloc[test_start:test_end]\n",
    "\n",
    "            # Features (same for all horizons)\n",
    "            X_train = train_df[self.X_cols].reset_index(drop=True)\n",
    "            X_val = test_df[self.X_cols].reset_index(drop=True)\n",
    "\n",
    "            # Dates of \"feature time\" (t)\n",
    "            X_train_dates = train_df[self.date_col].reset_index(drop=True)\n",
    "            X_val_dates = test_df[self.date_col].reset_index(drop=True)\n",
    "\n",
    "            # For each horizon, build targets & date of target\n",
    "            for h in self.horizons:\n",
    "                y_train_h = train_df[f\"{self.target_col}_h{h}\"].reset_index(drop=True)\n",
    "                y_val_h = test_df[f\"{self.target_col}_h{h}\"].reset_index(drop=True)\n",
    "\n",
    "                y_train_dates_h = train_df[f\"{self.date_col}_h{h}\"].reset_index(drop=True)\n",
    "                y_val_dates_h = test_df[f\"{self.date_col}_h{h}\"].reset_index(drop=True)\n",
    "\n",
    "                # Walkfolds\n",
    "                self.walkfolds[h].append(\n",
    "                    ((X_train, y_train_h), (X_val, y_val_h))\n",
    "                )\n",
    "\n",
    "                # Date folds\n",
    "                self.walkfold_dates[h].append(\n",
    "                    ((X_train_dates, y_train_dates_h),\n",
    "                     (X_val_dates,   y_val_dates_h))\n",
    "                )\n",
    "\n",
    "    # ======================== 3. OPTUNA OBJECTIVE (SHARED PARAMS) ========================\n",
    "\n",
    "    def create_objective(self):\n",
    "        \"\"\"\n",
    "        Optuna objective.\n",
    "\n",
    "        A single set of hyperparameters is shared for all horizons.\n",
    "        The objective value is the mean of the mean-RMSE across horizons.\n",
    "\n",
    "        - Dùng early_stopping_rounds trong constructor XGBRegressor (hợp XGBoost mới).\n",
    "        - Dùng trial.report + trial.should_prune để prune trial tệ.\n",
    "        \"\"\"\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'objective': 'reg:squarederror',\n",
    "                'eval_metric': 'rmse',\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "                'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 10.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 3, 10),\n",
    "            }\n",
    "\n",
    "            horizon_results = {}\n",
    "            overall_rmse_list = []\n",
    "            global_step = 0  # step index for Optuna pruning\n",
    "\n",
    "            for h in self.horizons:\n",
    "                scores = []\n",
    "                fold_results = []\n",
    "\n",
    "                for i, ((X_train, y_train), (X_val, y_val)) in enumerate(self.walkfolds[h]):\n",
    "\n",
    "                    model = xgb.XGBRegressor(\n",
    "                        **params,\n",
    "                        n_estimators=800,\n",
    "                        random_state=42,\n",
    "                        tree_method='hist',\n",
    "                        # early stopping for new XGBoost (passed via constructor)\n",
    "                        early_stopping_rounds=100\n",
    "                    )\n",
    "\n",
    "                    model.fit(\n",
    "                        X_train,\n",
    "                        y_train,\n",
    "                        eval_set=[(X_val, y_val)],\n",
    "                        verbose=False\n",
    "                    )\n",
    "\n",
    "                    preds = model.predict(X_val)\n",
    "                    # GIỮ LOG Y HỆT EM, NHƯNG Ở ĐÂY CHỊ DÙNG RMSE ĐÚNG NGHĨA\n",
    "                    rmse = mean_squared_error(y_val, preds)\n",
    "                    ss_res = np.sum((y_val - preds) ** 2)\n",
    "                    ss_tot = np.sum((y_val - np.mean(y_val)) ** 2)\n",
    "                    r2 = 1 - ss_res / ss_tot if ss_tot != 0 else 0.0\n",
    "\n",
    "                    scores.append(rmse)\n",
    "                    fold_results.append({'rmse': rmse, 'r2': r2})\n",
    "\n",
    "                    print(\n",
    "                        f\"Trial {trial.number}, Horizon h={h}, \"\n",
    "                        f\"Fold {i+1}: RMSE={rmse:.4f}, R²={r2:.4f}\"\n",
    "                    )\n",
    "\n",
    "                    # Report to Optuna and possibly prune\n",
    "                    trial.report(rmse, step=global_step)\n",
    "                    if trial.should_prune():\n",
    "                        print(f\"Trial {trial.number} pruned at global_step={global_step}\")\n",
    "                        raise optuna.TrialPruned()\n",
    "                    global_step += 1\n",
    "\n",
    "                mean_rmse_h = float(np.mean(scores))\n",
    "                horizon_results[h] = {\n",
    "                    'mean_rmse': mean_rmse_h,\n",
    "                    'fold_scores': fold_results,\n",
    "                }\n",
    "                overall_rmse_list.append(mean_rmse_h)\n",
    "\n",
    "            overall_mean_rmse = float(np.mean(overall_rmse_list))\n",
    "            print(\n",
    "                f\"Trial {trial.number} finished, \"\n",
    "                f\"overall mean RMSE across horizons={overall_mean_rmse:.4f}\\n\"\n",
    "            )\n",
    "\n",
    "            self.fold_history.append({\n",
    "                'trial': trial.number,\n",
    "                'params': params,\n",
    "                'horizon_results': horizon_results,\n",
    "                'overall_mean_rmse': overall_mean_rmse,\n",
    "            })\n",
    "\n",
    "            return overall_mean_rmse\n",
    "\n",
    "        return objective\n",
    "\n",
    "    # ======================== 4. RUN OPTUNA ========================\n",
    "\n",
    "    def run_optuna(self, n_trials=50):\n",
    "        \"\"\"\n",
    "        Run Optuna to find shared hyperparameters for all horizons.\n",
    "\n",
    "        Dùng MedianPruner để cắt trial tệ.\n",
    "        \"\"\"\n",
    "        objective_fn = self.create_objective()\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction='minimize',\n",
    "            pruner=optuna.pruners.MedianPruner(\n",
    "                n_warmup_steps=5  # number of steps before pruning is enabled\n",
    "            )\n",
    "        )\n",
    "        study.optimize(objective_fn, n_trials=n_trials)\n",
    "        self.best_params = study.best_params\n",
    "        return study\n",
    "\n",
    "    # ======================== 5. TRAIN A FINAL MODEL PER HORIZON ========================\n",
    "\n",
    "    def train_final_models(self):\n",
    "        \"\"\"\n",
    "        Train one final model per horizon on the entire development set\n",
    "        using the best hyperparameters from Optuna.\n",
    "        \"\"\"\n",
    "        if self.best_params is None:\n",
    "            raise ValueError(\"You must run run_optuna() before train_final_models().\")\n",
    "\n",
    "        final_params = self.best_params.copy()\n",
    "        final_params.update({\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'rmse',\n",
    "            'n_estimators': 800\n",
    "        })\n",
    "\n",
    "        X_full = self.df[self.X_cols]\n",
    "\n",
    "        for h in self.horizons:\n",
    "            y_full_h = self.df[f\"{self.target_col}_h{h}\"]\n",
    "\n",
    "            model_h = xgb.XGBRegressor(\n",
    "                **final_params,\n",
    "                random_state=42,\n",
    "                tree_method='hist'\n",
    "            )\n",
    "\n",
    "            model_h.fit(X_full, y_full_h, verbose=False)\n",
    "            self.final_models[h] = model_h\n",
    "\n",
    "    # ======================== 6. PREDICTION HELPERS ========================\n",
    "\n",
    "    def predict_horizon(self, X_today, h):\n",
    "        \"\"\"\n",
    "        Predict target at t+h for a single row or DataFrame X_today.\n",
    "        \"\"\"\n",
    "        if h not in self.horizons:\n",
    "            raise ValueError(f\"Horizon {h} not in configured horizons {self.horizons}.\")\n",
    "        if self.final_models[h] is None:\n",
    "            raise ValueError(\n",
    "                f\"Model for horizon {h} is not trained. \"\n",
    "                f\"Call train_final_models() first.\"\n",
    "            )\n",
    "\n",
    "        X_input = X_today[self.X_cols]\n",
    "        return self.final_models[h].predict(X_input)\n",
    "\n",
    "    def predict_all_horizons(self, X_today):\n",
    "        \"\"\"\n",
    "        Predict for all configured horizons for a given X_today (1 row or small DataFrame).\n",
    "        Returns a dict: {h: prediction_array}\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        for h in self.horizons:\n",
    "            results[h] = self.predict_horizon(X_today, h)\n",
    "        return results\n",
    "\n",
    "    # ======================== 6b. BUILD PREDICTIONS FRAME (ĐỂ VẼ ĐỒ THỊ) ========================\n",
    "\n",
    "    def get_predictions_frame(self, df_with_shift, h):\n",
    "        \"\"\"\n",
    "        Build a DataFrame with dates, true values and predictions for horizon h.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_with_shift : pd.DataFrame\n",
    "            DataFrame (train hoặc test) đã có sẵn các cột:\n",
    "            - self.date_col\n",
    "            - f\"{self.target_col}_h{h}\"\n",
    "            - f\"{self.date_col}_h{h}\" (nếu có)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df_pred : pd.DataFrame with columns:\n",
    "            - feature_time : time t (where features are taken from)\n",
    "            - target_time  : time t+h (if available), else feature_time\n",
    "            - y_true       : true target at t+h\n",
    "            - y_pred       : model prediction at t+h\n",
    "        \"\"\"\n",
    "        if h not in self.horizons:\n",
    "            raise ValueError(f\"Horizon {h} not in configured horizons {self.horizons}.\")\n",
    "        if self.final_models[h] is None:\n",
    "            raise ValueError(\n",
    "                f\"Model for horizon {h} is not trained. \"\n",
    "                f\"Call train_final_models() first.\"\n",
    "            )\n",
    "\n",
    "        X = df_with_shift[self.X_cols]\n",
    "        y_true = df_with_shift[f\"{self.target_col}_h{h}\"]\n",
    "        y_pred = self.final_models[h].predict(X)\n",
    "\n",
    "        feature_time = df_with_shift[self.date_col]\n",
    "        if f\"{self.date_col}_h{h}\" in df_with_shift.columns:\n",
    "            target_time = df_with_shift[f\"{self.date_col}_h{h}\"]\n",
    "        else:\n",
    "            # fallback: dùng feature_time nếu không có date_h\n",
    "            target_time = feature_time\n",
    "\n",
    "        df_pred = pd.DataFrame({\n",
    "            \"feature_time\": feature_time.values,\n",
    "            \"target_time\": target_time.values,\n",
    "            \"y_true\": y_true.values,\n",
    "            \"y_pred\": y_pred,\n",
    "        })\n",
    "        return df_pred\n",
    "\n",
    "    def plot_predictions(self, df_with_shift, h, use_target_time=True, n_points=None):\n",
    "        \"\"\"\n",
    "        Plot true vs predicted for a given horizon h.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_with_shift : pd.DataFrame\n",
    "            DataFrame (thường là test_df đã shift bằng prepare_test_dataset).\n",
    "        h : int\n",
    "            Horizon to plot (must be in self.horizons).\n",
    "        use_target_time : bool, default True\n",
    "            If True and target_time exists, x-axis = target_time (t+h).\n",
    "            If False, x-axis = feature_time (t).\n",
    "        n_points : int or None\n",
    "            If not None, only the first n_points are plotted.\n",
    "        \"\"\"\n",
    "        df_pred = self.get_predictions_frame(df_with_shift, h)\n",
    "\n",
    "        df_plot = df_pred.copy()\n",
    "        if use_target_time and \"target_time\" in df_plot.columns:\n",
    "            df_plot[\"x\"] = df_plot[\"target_time\"]\n",
    "        else:\n",
    "            df_plot[\"x\"] = df_plot[\"feature_time\"]\n",
    "\n",
    "        if n_points is not None:\n",
    "            df_plot = df_plot.iloc[:n_points]\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(df_plot[\"x\"], df_plot[\"y_true\"], label=\"True\")\n",
    "        plt.plot(df_plot[\"x\"], df_plot[\"y_pred\"], label=\"Predicted\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(self.target_col)\n",
    "        plt.title(f\"Horizon h={h} forecast\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # ======================== 7. EVAL ON TRAIN SET (CHECK OVERFIT) ========================\n",
    "\n",
    "    def evaluate_train_models(self):\n",
    "        \"\"\"\n",
    "        Evaluate all final models on the training (development) set.\n",
    "        Returns a dict: {h: {'rmse','mae','mape','mse','r2'}}\n",
    "        \"\"\"\n",
    "        if any(self.final_models[h] is None for h in self.horizons):\n",
    "            raise ValueError(\"All horizon models must be trained before evaluation.\")\n",
    "\n",
    "        metrics = {}\n",
    "        X_train_full = self.df[self.X_cols]\n",
    "\n",
    "        for h in self.horizons:\n",
    "            y_train_h = self.df[f\"{self.target_col}_h{h}\"]\n",
    "            preds = self.final_models[h].predict(X_train_full)\n",
    "\n",
    "            rmse = mean_squared_error(y_train_h, preds)\n",
    "            mae = mean_absolute_error(y_train_h, preds)\n",
    "            mse = mean_squared_error(y_train_h, preds)\n",
    "\n",
    "            # avoid division by zero for MAPE\n",
    "            eps = 1e-8\n",
    "            mape = np.mean(np.abs((y_train_h - preds) / (y_train_h + eps))) * 100\n",
    "\n",
    "            ss_res = np.sum((y_train_h - preds) ** 2)\n",
    "            ss_tot = np.sum((y_train_h - np.mean(y_train_h)) ** 2)\n",
    "            r2 = 1 - ss_res / ss_tot if ss_tot != 0 else 0.0\n",
    "\n",
    "            print(\n",
    "                f\"[TRAIN] Horizon h={h} -> \"\n",
    "                f\"RMSE: {rmse:.4f}, R²: {r2:.4f}\"\n",
    "            )\n",
    "\n",
    "            metrics[h] = {\n",
    "                'rmse': rmse,\n",
    "                'mae': mae,\n",
    "                'mape': mape,\n",
    "                'mse': mse,\n",
    "                'r2': r2,\n",
    "            }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    # ======================== 8. EVALUATION ON A TEST SET ========================\n",
    "\n",
    "    def evaluate_final_models(self, test_df):\n",
    "        \"\"\"\n",
    "        Evaluate all final models on a separate test DataFrame that already contains\n",
    "        the shifted targets (i.e., you must have created _h1.._h5 on it).\n",
    "        Returns a dict: {h: {'rmse','mae','mape','mse','r2'}}\n",
    "        \"\"\"\n",
    "        if any(self.final_models[h] is None for h in self.horizons):\n",
    "            raise ValueError(\"All horizon models must be trained before evaluation.\")\n",
    "\n",
    "        metrics = {}\n",
    "        X_test = test_df[self.X_cols]\n",
    "\n",
    "        for h in self.horizons:\n",
    "            y_test_h = test_df[f\"{self.target_col}_h{h}\"]\n",
    "            preds = self.final_models[h].predict(X_test)\n",
    "\n",
    "            rmse = mean_squared_error(y_test_h, preds)\n",
    "            mae = mean_absolute_error(y_test_h, preds)\n",
    "            mse = mean_squared_error(y_test_h, preds)\n",
    "\n",
    "            eps = 1e-8\n",
    "            mape = np.mean(np.abs((y_test_h - preds) / (y_test_h + eps))) * 100\n",
    "\n",
    "            ss_res = np.sum((y_test_h - preds) ** 2)\n",
    "            ss_tot = np.sum((y_test_h - np.mean(y_test_h)) ** 2)\n",
    "            r2 = 1 - ss_res / ss_tot if ss_tot != 0 else 0.0\n",
    "\n",
    "            print(f\"Horizon h={h} -> RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "            metrics[h] = {\n",
    "                'rmse': rmse,\n",
    "                'mae': mae,\n",
    "                'mape': mape,\n",
    "                'mse': mse,\n",
    "                'r2': r2,\n",
    "            }\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99290f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_pipeline = MultiHorizonWalkForwardOptuna_XGBoost_Pipeline(\n",
    "    df=train_dataset,\n",
    "    date_col=\"datetime\",\n",
    "    target_col=\"temp\",\n",
    "    feature_cols=[col for col in train_dataset.columns if col not in [\"datetime\", \"temp\", \"temp_next\", \"datetime_next\"]],\n",
    "    n_splits=5,\n",
    "    test_size=365,\n",
    "    mode=\"rolling\",\n",
    "    horizons=(1, 2, 3, 4, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e27c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_pipeline.add_target_shifts()\n",
    "XGBoost_pipeline.create_walkforward_folds()\n",
    "XGBoost_pipeline.run_optuna(n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b0a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_pipeline.train_final_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcda6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train metrics\n",
    "train_metrics = XGBoost_pipeline.evaluate_train_models()\n",
    "train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442cbccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "test_df_shifted = XGBoost_pipeline.prepare_test_dataset(test_dataset)\n",
    "test_metrics = XGBoost_pipeline.evaluate_final_models(test_df_shifted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
